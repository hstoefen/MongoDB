{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 1 - Data Extraction Fundamentals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beatles Diskography"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Title': 'Please Please Me', 'UK Chart Position': '1', 'Label': 'Parlophone(UK)', 'Released': '22 March 1963', 'US Chart Position': '-', 'RIAA Certification': 'Platinum', 'BPI Certification': 'Gold'}\n",
      "{'Title': '', 'UK Chart Position': '1', 'Label': 'Parlophone(UK)', 'Released': '10 July 1964', 'US Chart Position': '-', 'RIAA Certification': '', 'BPI Certification': 'Gold'}\n"
     ]
    }
   ],
   "source": [
    "# Your task is to read the input DATAFILE line by line, and for the first 10 lines (not including the header)\n",
    "# split each line on \",\" and then for each line, create a dictionary\n",
    "# where the key is the header title of the field, and the value is the value of that field in the row.\n",
    "# The function parse_file should return a list of dictionaries,\n",
    "# each data line in the file being a single list entry.\n",
    "# Field names and values should not contain extra whitespace, like spaces or newline characters.\n",
    "# You can use the Python string method strip() to remove the extra whitespace.\n",
    "# You have to parse only the first 10 data lines in this exercise,\n",
    "# so the returned list should have 10 entries!\n",
    "import os\n",
    "\n",
    "DATADIR = \"data\"\n",
    "DATAFILE = \"beatles-diskography.csv\"\n",
    "\n",
    "\n",
    "def parse_file(datafile):\n",
    "    data = []\n",
    "    rows = []\n",
    "    with open(datafile, \"r\") as f:\n",
    "        for line in f:        \n",
    "            line = line.split(',')\n",
    "            for i in range(len(line)):\n",
    "                line[i] = line[i].strip()\n",
    "            rows.append(line)\n",
    "        for j in range(10):\n",
    "            data.append(dict(zip(rows[0],rows[j+1])))\n",
    "        \n",
    "        print data[0]\n",
    "        print data[9]\n",
    "    return data\n",
    "\n",
    "\n",
    "def test():\n",
    "    # a simple test of your implemetation\n",
    "    datafile = os.path.join(DATADIR, DATAFILE)\n",
    "    d = parse_file(datafile)\n",
    "    #print d\n",
    "    firstline = {'Title': 'Please Please Me', 'UK Chart Position': '1', 'Label': 'Parlophone(UK)', 'Released': '22 March 1963', 'US Chart Position': '-', 'RIAA Certification': 'Platinum', 'BPI Certification': 'Gold'}\n",
    "    tenthline = {'Title': '', 'UK Chart Position': '1', 'Label': 'Parlophone(UK)', 'Released': '10 July 1964', 'US Chart Position': '-', 'RIAA Certification': '', 'BPI Certification': 'Gold'}\n",
    "\n",
    "    assert d[0] == firstline\n",
    "    assert d[9] == tenthline\n",
    "    \n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## ERCOT Hourly Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18779.02551\n",
      "5392\n",
      "(2013, 8, 13, 17, 0, 0)\n",
      "6602.113899\n",
      "(2013, 2, 3, 4, 0, 0)\n",
      "10976.9334607\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "\"\"\"\n",
    "Your task is as follows:\n",
    "- read the provided Excel file\n",
    "- find and return the min, max and average values for the COAST region\n",
    "- find and return the time value for the min and max entries\n",
    "- the time values should be returned as Python tuples\n",
    "\n",
    "Please see the test function for the expected return format\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import xlrd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from zipfile import ZipFile\n",
    "datafile = \"./data/2013_ERCOT_Hourly_Load_Data.xls\"\n",
    "\n",
    "\n",
    "def open_zip(datafile):\n",
    "    with ZipFile(datafile, 'r') as myzip:\n",
    "        myzip.extractall()\n",
    "\n",
    "\n",
    "def parse_file(datafile):\n",
    "    workbook = xlrd.open_workbook(datafile)\n",
    "    sheet = workbook.sheet_by_index(0)\n",
    "\n",
    "    ### example on how you can get the data\n",
    "    sheet_data = pd.DataFrame([[sheet.cell_value(r, col) for col in range(sheet.ncols)] for r in range(sheet.nrows)])\n",
    "\n",
    "    maxcoast = sheet_data[1][1:].max()\n",
    "    print maxcoast\n",
    "    maxcoastidx = sheet_data[1][1:].idxmax()\n",
    "    print maxcoastidx\n",
    "    maxtime = xlrd.xldate_as_tuple(sheet.cell_value(maxcoastidx, 0), 0)\n",
    "    print maxtime\n",
    "    \n",
    "    mincoast = sheet_data[1][1:].min()\n",
    "    print mincoast\n",
    "    mincoastidx = sheet_data[1][1:].idxmin()\n",
    "    mintime = xlrd.xldate_as_tuple(sheet.cell_value(mincoastidx, 0), 0)\n",
    "    print mintime\n",
    "    \n",
    "    meancoast = sheet_data[1][1:].mean()\n",
    "    print meancoast\n",
    "\n",
    "    ### other useful methods:\n",
    "    #print \"\\nROWS, COLUMNS, and CELLS:\"\n",
    "    #print \"Number of rows in the sheet:\", \n",
    "    #print sheet.nrows\n",
    "    #print \"Type of data in cell (row 3, col 2):\", \n",
    "    #print sheet.cell_type(3, 2)\n",
    "    #print \"Value in cell (row 3, col 2):\", \n",
    "    #print sheet.cell_value(3, 2)\n",
    "    #print \"Get a slice of values in column 3, from rows 1-3:\"\n",
    "    #print sheet.col_values(3, start_rowx=1, end_rowx=4)\n",
    "\n",
    "    # print \"\\nDATES:\"\n",
    "    # print \"Type of data in cell (row 1, col 0):\", \n",
    "    # print sheet.cell_type(1, 0)\n",
    "    # exceltime = sheet.cell_value(1, 0)\n",
    "    # print \"Time in Excel format:\",\n",
    "    # print exceltime\n",
    "    # print \"Convert time to a Python datetime tuple, from the Excel float:\",\n",
    "    # print xlrd.xldate_as_tuple(exceltime, 0)\n",
    "    \n",
    "    \n",
    "    data = {\n",
    "            'maxtime': maxtime,\n",
    "            'maxvalue': maxcoast,\n",
    "            'mintime': mintime,\n",
    "            'minvalue': mincoast,\n",
    "            'avgcoast': meancoast\n",
    "    }\n",
    "    return data\n",
    "\n",
    "\n",
    "def test():\n",
    "    #open_zip(datafile)\n",
    "    data = parse_file(datafile)\n",
    "\n",
    "    assert data['maxtime'] == (2013, 8, 13, 17, 0, 0)\n",
    "    assert round(data['maxvalue'], 10) == round(18779.02551, 10)\n",
    "\n",
    "\n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Musicbrainz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "requesting http://musicbrainz.org/ws/2/artist/?query=artist%3ANirvana&fmt=json\n",
      "requesting http://musicbrainz.org/ws/2/artist/5b11f4ce-a62d-471e-81fc-a69a8278c7da?fmt=json&inc=releases\n",
      "\n",
      "ALL TITLES:\n",
      "Never Mind the Bollocks Here’s Nirvana\n",
      "Blew\n",
      "Sliver\n",
      "Bleach\n",
      "Sliver\n",
      "Love Buzz / Big Cheese\n",
      "Smells Like Teen Spirit\n",
      "Smells Like Teen Spirit\n",
      "Here She Comes Now / Venus in Furs\n",
      "Smells Like Teen Spirit\n",
      "Smells Like Teen Spirit\n",
      "Bleach\n",
      "Sliver\n",
      "Smells Like Teen Spirit\n",
      "Blew\n",
      "Candy / Molly’s Lips\n",
      "Bleach\n",
      "Sliver\n",
      "Smells Like Teen Spirit\n",
      "Nevermind\n",
      "Bleach\n",
      "Smells Like Teen Spirit\n",
      "Bleach\n",
      "Sliver\n",
      "Nevermind\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "To experiment with this code freely you will have to run this code locally.\n",
    "Take a look at the main() function for an example of how to use the code. We\n",
    "have provided example json output in the other code editor tabs for you to look\n",
    "at, but you will not be able to run any queries through our UI.\n",
    "\"\"\"\n",
    "import json\n",
    "import requests\n",
    "\n",
    "BASE_URL = \"http://musicbrainz.org/ws/2/\"\n",
    "ARTIST_URL = BASE_URL + \"artist/\"\n",
    "\n",
    "\n",
    "# query parameters are given to the requests.get function as a dictionary; this\n",
    "# variable contains some starter parameters.\n",
    "query_type = {  \"simple\": {},\n",
    "                \"atr\": {\"inc\": \"aliases+tags+ratings\"},\n",
    "                \"aliases\": {\"inc\": \"aliases\"},\n",
    "                \"releases\": {\"inc\": \"releases\"}}\n",
    "\n",
    "\n",
    "def query_site(url, params, uid=\"\", fmt=\"json\"):\n",
    "    \"\"\"\n",
    "    This is the main function for making queries to the musicbrainz API. The\n",
    "    query should return a json document.\n",
    "    \"\"\"\n",
    "    params[\"fmt\"] = fmt\n",
    "    r = requests.get(url + uid, params=params)\n",
    "    print \"requesting\", r.url\n",
    "\n",
    "    if r.status_code == requests.codes.ok:\n",
    "        return r.json()\n",
    "    else:\n",
    "        r.raise_for_status()\n",
    "\n",
    "\n",
    "def query_by_name(url, params, name):\n",
    "    \"\"\"\n",
    "    This adds an artist name to the query parameters before making an API call\n",
    "    to the function above.\n",
    "    \"\"\"\n",
    "    params[\"query\"] = \"artist:\" + name\n",
    "    return query_site(url, params)\n",
    "\n",
    "\n",
    "def pretty_print(data, indent=4):\n",
    "    \"\"\"\n",
    "    After we get our output, we can use this function to format it to be more\n",
    "    readable.\n",
    "    \"\"\"\n",
    "    if type(data) == dict:\n",
    "        print json.dumps(data, indent=indent, sort_keys=True)\n",
    "    else:\n",
    "        print data\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Below is an example investigation to help you get started in your\n",
    "    exploration. Modify the function calls and indexing below to answer the\n",
    "    questions on the next quiz.\n",
    "\n",
    "    HINT: Note how the output we get from the site is a multi-level JSON\n",
    "    document, so try making print statements to step through the structure one\n",
    "    level at a time or copy the output to a separate output file. Experimenting\n",
    "    and iteration will be key to understand the structure of the data!\n",
    "    \"\"\"\n",
    "\n",
    "    # Query for information in the database about bands named Nirvana\n",
    "    results = query_by_name(ARTIST_URL, query_type[\"simple\"], \"Nirvana\")\n",
    "    #pretty_print(results)\n",
    "\n",
    "    # Isolate information from the 4th band returned (index 3)\n",
    "    #print \"\\nARTIST:\"\n",
    "    #pretty_print(results[\"artists\"][4])\n",
    "\n",
    "    # Query for releases from that band using the artist_id\n",
    "    artist_id = results[\"artists\"][4][\"id\"]\n",
    "    artist_data = query_site(ARTIST_URL, query_type[\"releases\"], artist_id)\n",
    "    releases = artist_data[\"releases\"]\n",
    "\n",
    "    # Print information about releases from the selected band\n",
    "    #print \"\\nONE RELEASE:\"\n",
    "    #pretty_print(releases[6], indent=2)\n",
    "\n",
    "    release_titles = [r[\"title\"] for r in releases]\n",
    "    print \"\\nALL TITLES:\"\n",
    "    for t in release_titles:\n",
    "        print t\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quiz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "requesting http://musicbrainz.org/ws/2/artist/?query=artist%3AOne+Direction&fmt=json\n",
      "{\n",
      "    \"aliases\": [\n",
      "        {\n",
      "            \"begin-date\": null, \n",
      "            \"end-date\": null, \n",
      "            \"locale\": null, \n",
      "            \"name\": \"1D\", \n",
      "            \"primary\": null, \n",
      "            \"sort-name\": \"1D\", \n",
      "            \"type\": \"Artist name\"\n",
      "        }\n",
      "    ], \n",
      "    \"area\": {\n",
      "        \"id\": \"8a754a16-0027-3a29-b6d7-2b40ea0481ed\", \n",
      "        \"name\": \"United Kingdom\", \n",
      "        \"sort-name\": \"United Kingdom\"\n",
      "    }, \n",
      "    \"begin-area\": {\n",
      "        \"id\": \"f03d09b3-39dc-4083-afd6-159e3f0d462f\", \n",
      "        \"name\": \"London\", \n",
      "        \"sort-name\": \"London\"\n",
      "    }, \n",
      "    \"country\": \"GB\", \n",
      "    \"id\": \"1a425bbd-cca4-4b2c-aeb7-71cb176c828a\", \n",
      "    \"life-span\": {\n",
      "        \"begin\": \"2010-07\", \n",
      "        \"ended\": null\n",
      "    }, \n",
      "    \"name\": \"One Direction\", \n",
      "    \"score\": \"100\", \n",
      "    \"sort-name\": \"One Direction\", \n",
      "    \"tags\": [\n",
      "        {\n",
      "            \"count\": 2, \n",
      "            \"name\": \"pop\"\n",
      "        }, \n",
      "        {\n",
      "            \"count\": 1, \n",
      "            \"name\": \"power pop\"\n",
      "        }, \n",
      "        {\n",
      "            \"count\": 1, \n",
      "            \"name\": \"dance-pop\"\n",
      "        }, \n",
      "        {\n",
      "            \"count\": 1, \n",
      "            \"name\": \"pop rock\"\n",
      "        }, \n",
      "        {\n",
      "            \"count\": 1, \n",
      "            \"name\": \"folk pop\"\n",
      "        }, \n",
      "        {\n",
      "            \"count\": 2, \n",
      "            \"name\": \"boy band\"\n",
      "        }\n",
      "    ], \n",
      "    \"type\": \"Group\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "To experiment with this code freely you will have to run this code locally.\n",
    "Take a look at the main() function for an example of how to use the code. We\n",
    "have provided example json output in the other code editor tabs for you to look\n",
    "at, but you will not be able to run any queries through our UI.\n",
    "\"\"\"\n",
    "import json\n",
    "import requests\n",
    "\n",
    "BASE_URL = \"http://musicbrainz.org/ws/2/\"\n",
    "ARTIST_URL = BASE_URL + \"artist/\"\n",
    "\n",
    "\n",
    "# query parameters are given to the requests.get function as a dictionary; this\n",
    "# variable contains some starter parameters.\n",
    "query_type = {  \"simple\": {},\n",
    "                \"atr\": {\"inc\": \"aliases+tags+ratings\"},\n",
    "                \"aliases\": {\"inc\": \"aliases\"},\n",
    "                \"releases\": {\"inc\": \"releases\"}}\n",
    "\n",
    "\n",
    "def query_site(url, params, uid=\"\", fmt=\"json\"):\n",
    "    \"\"\"\n",
    "    This is the main function for making queries to the musicbrainz API. The\n",
    "    query should return a json document.\n",
    "    \"\"\"\n",
    "    params[\"fmt\"] = fmt\n",
    "    r = requests.get(url + uid, params=params)\n",
    "    print \"requesting\", r.url\n",
    "\n",
    "    if r.status_code == requests.codes.ok:\n",
    "        return r.json()\n",
    "    else:\n",
    "        r.raise_for_status()\n",
    "\n",
    "\n",
    "def query_by_name(url, params, name):\n",
    "    \"\"\"\n",
    "    This adds an artist name to the query parameters before making an API call\n",
    "    to the function above.\n",
    "    \"\"\"\n",
    "    params[\"query\"] = \"artist:\" + name\n",
    "    return query_site(url, params)\n",
    "\n",
    "\n",
    "def pretty_print(data, indent=4):\n",
    "    \"\"\"\n",
    "    After we get our output, we can use this function to format it to be more\n",
    "    readable.\n",
    "    \"\"\"\n",
    "    if type(data) == dict:\n",
    "        print json.dumps(data, indent=indent, sort_keys=True)\n",
    "    else:\n",
    "        print data\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Below is an example investigation to help you get started in your\n",
    "    exploration. Modify the function calls and indexing below to answer the\n",
    "    questions on the next quiz.\n",
    "\n",
    "    HINT: Note how the output we get from the site is a multi-level JSON\n",
    "    document, so try making print statements to step through the structure one\n",
    "    level at a time or copy the output to a separate output file. Experimenting\n",
    "    and iteration will be key to understand the structure of the data!\n",
    "    \"\"\"\n",
    "\n",
    "    # Query for information in the database about bands named First Aid Kit, Queen, Beatles, Nirvana One Direction\n",
    "    results = query_by_name(ARTIST_URL, query_type[\"simple\"], \"One Direction\")\n",
    "    #pretty_print(results)\n",
    "\n",
    "    # Determine number of bands named First Aid Kit\n",
    "    #print \"\\nNumber of artists named First Aid Kit:\"\n",
    "    #nFAK = 0\n",
    "    #for n in results[\"artists\"]:\n",
    "    #    if n['name'] == 'First Aid Kit':\n",
    "    #        nFAK += 1\n",
    "    #print nFAK\n",
    "    \n",
    "    # Determine begin-area name for Queen\n",
    "    #print results[\"artists\"][2][\"begin-area\"][\"name\"]\n",
    "    \n",
    "    # Determine Spanish alias for Beatles\n",
    "    #for n in results[\"artists\"][8]['aliases']:\n",
    "    #    if n['locale'] == 'es':\n",
    "    #        print n['name']\n",
    "    \n",
    "    # Determine disambiguation for Nirvana\n",
    "    #print results['artists'][4]['disambiguation']\n",
    "    \n",
    "    # Determine, when One Direction was formed\n",
    "    #print results['artists'][0]['life-span']['begin']\n",
    "    pretty_print(results['artists'][0])\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Lesson 2 - Problem Set: Data Extraction Fundamentals\n",
    "## Solar and Wind Energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\"\"\"\n",
    "Your task is to process the supplied file and use the csv module to extract data from it.\n",
    "The data comes from NREL (National Renewable Energy Laboratory) website. Each file\n",
    "contains information from one meteorological station, in particular - about amount of\n",
    "solar and wind energy for each hour of day.\n",
    "\n",
    "Note that the first line of the datafile is neither data entry, nor header. It is a line\n",
    "describing the data source. You should extract the name of the station from it.\n",
    "\n",
    "The data should be returned as a list of lists (not dictionaries).\n",
    "You can use the csv modules \"reader\" method to get data in such format.\n",
    "Another useful method is next() - to get the next line from the iterator.\n",
    "You should only change the parse_file function.\n",
    "\"\"\"\n",
    "import csv\n",
    "import os\n",
    "\n",
    "DATADIR = \"data\"\n",
    "DATAFILE = \"745090.csv\"\n",
    "\n",
    "\n",
    "def parse_file(datafile):\n",
    "    name = \"\"\n",
    "    data = []\n",
    "    with open(datafile,'rb') as f:\n",
    "        reader = csv.reader(f, delimiter=',')\n",
    "        for row in reader:\n",
    "            data.append(row)\n",
    "        #    print row\n",
    "    name = data[0][1]\n",
    "    data = data[2:]\n",
    "    \n",
    "    # Do not change the line below\n",
    "    return (name, data)\n",
    "\n",
    "\n",
    "def test():\n",
    "    datafile = os.path.join(DATADIR, DATAFILE)\n",
    "    name, data = parse_file(datafile)\n",
    "\n",
    "    assert name == \"MOUNTAIN VIEW MOFFETT FLD NAS\"\n",
    "    assert data[0][1] == \"01:00\"\n",
    "    assert data[2][0] == \"01/01/2005\"\n",
    "    assert data[2][5] == \"2\"\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time and Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[((2013, 8, 13, 17, 0, 0), 18779.025510000003), ((2013, 8, 5, 17, 0, 0), 2380.1654089999956), ((2013, 6, 26, 17, 0, 0), 2281.2722140000024), ((2013, 8, 7, 17, 0, 0), 1544.7707140000005), ((2013, 8, 7, 18, 0, 0), 24415.570226999993), ((2013, 8, 8, 16, 0, 0), 5494.157645), ((2013, 8, 8, 18, 0, 0), 11433.30491600001), ((2013, 8, 7, 17, 0, 0), 1862.6137649999998)]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-d838079f0435>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"__main__\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 96\u001b[1;33m     \u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-8-d838079f0435>\u001b[0m in \u001b[0;36mtest\u001b[1;34m()\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m         \u001b[1;31m# Output should be 8 lines not including header\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 89\u001b[1;33m         \u001b[1;32massert\u001b[0m \u001b[0mnumber_of_rows\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m8\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     90\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m         \u001b[1;31m# Check Station Names\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "'''\n",
    "Find the time and value of max load for each of the regions\n",
    "COAST, EAST, FAR_WEST, NORTH, NORTH_C, SOUTHERN, SOUTH_C, WEST\n",
    "and write the result out in a csv file, using pipe character | as the delimiter.\n",
    "\n",
    "An example output can be seen in the \"example.csv\" file.\n",
    "'''\n",
    "\n",
    "import xlrd\n",
    "import os\n",
    "import csv\n",
    "from zipfile import ZipFile\n",
    "\n",
    "datafile = \"./data/2013_ERCOT_Hourly_Load_Data.xls\"\n",
    "outfile = \"./data/2013_Max_Loads.csv\"\n",
    "\n",
    "\n",
    "def open_zip(datafile):\n",
    "    with ZipFile('{0}.zip'.format(datafile), 'r') as myzip:\n",
    "        myzip.extractall()\n",
    "\n",
    "\n",
    "def parse_file(datafile):\n",
    "    workbook = xlrd.open_workbook(datafile)\n",
    "    sheet = workbook.sheet_by_index(0)\n",
    "    data = []\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    # Remember that you can use xlrd.xldate_as_tuple(sometime, 0) to convert\n",
    "    # Excel date to Python tuple of (year, month, day, hour, minute, second)\n",
    "\n",
    "    num_cols = sheet.ncols   # Number of columns\n",
    "    for col_idx in range(1, num_cols-1):    # Iterate through columns\n",
    "        col_max = 0\n",
    "        for row_idx in range(1, sheet.nrows):  # Iterate through rows\n",
    "            cell_obj = sheet.cell(row_idx, col_idx)  # Get cell object by row, col\n",
    "            if cell_obj.value > col_max:\n",
    "                col_max = cell_obj.value\n",
    "                max_time = xlrd.xldate_as_tuple(sheet.cell(row_idx, 0).value, 0)\n",
    "                #print max_time\n",
    "        data.append((max_time,col_max))\n",
    "    print data\n",
    "    return data\n",
    "\n",
    "def save_file(data, filename):\n",
    "    # YOUR CODE HERE\n",
    "    with open(filename, 'wb') as csvfile:\n",
    "        writer = csv.writer(csvfile, delimiter='|')\n",
    "        writer.writerow(data)\n",
    "    \n",
    "def test():\n",
    "    open_zip(datafile)\n",
    "    data = parse_file(datafile)\n",
    "    save_file(data, outfile)\n",
    "\n",
    "    number_of_rows = 0\n",
    "    stations = []\n",
    "\n",
    "    ans = {'FAR_WEST': {'Max Load': '2281.2722140000024',\n",
    "                        'Year': '2013',\n",
    "                        'Month': '6',\n",
    "                        'Day': '26',\n",
    "                        'Hour': '17'}}\n",
    "    correct_stations = ['COAST', 'EAST', 'FAR_WEST', 'NORTH',\n",
    "                        'NORTH_C', 'SOUTHERN', 'SOUTH_C', 'WEST']\n",
    "    fields = ['Year', 'Month', 'Day', 'Hour', 'Max Load']\n",
    "\n",
    "    with open(outfile) as of:\n",
    "        csvfile = csv.DictReader(of, delimiter=\"|\")\n",
    "        for line in csvfile:\n",
    "            station = line['Station']\n",
    "            if station == 'FAR_WEST':\n",
    "                for field in fields:\n",
    "                    # Check if 'Max Load' is within .1 of answer\n",
    "                    if field == 'Max Load':\n",
    "                        max_answer = round(float(ans[station][field]), 1)\n",
    "                        max_line = round(float(line[field]), 1)\n",
    "                        assert max_answer == max_line\n",
    "\n",
    "                    # Otherwise check for equality\n",
    "                    else:\n",
    "                        assert ans[station][field] == line[field]\n",
    "\n",
    "            number_of_rows += 1\n",
    "            stations.append(station)\n",
    "\n",
    "        # Output should be 8 lines not including header\n",
    "        assert number_of_rows == 8\n",
    "\n",
    "        # Check Station Names\n",
    "        assert set(stations) == set(correct_stations)\n",
    "\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Lesson 3 - Data in More Complex Formats\n",
    "## Biomed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fnm': 'Omer', 'snm': 'Mei-Dan', 'email': 'omer@extremegate.com'}\n",
      "{'fnm': 'Mike', 'snm': 'Carmont', 'email': 'mcarmont@hotmail.com'}\n",
      "{'fnm': 'Lior', 'snm': 'Laver', 'email': 'laver17@gmail.com'}\n",
      "{'fnm': 'Meir', 'snm': 'Nyska', 'email': 'nyska@internet-zahav.net'}\n",
      "{'fnm': 'Hagay', 'snm': 'Kammar', 'email': 'kammarh@gmail.com'}\n",
      "{'fnm': 'Gideon', 'snm': 'Mann', 'email': 'gideon.mann.md@gmail.com'}\n",
      "{'fnm': 'Barnaby', 'snm': 'Clarck', 'email': 'barns.nz@gmail.com'}\n",
      "{'fnm': 'Eugene', 'snm': 'Kots', 'email': 'eukots@gmail.com'}\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# Your task here is to extract data from xml on authors of an article\n",
    "# and add it to a list, one item for an author.\n",
    "# See the provided data structure for the expected format.\n",
    "# The tags for first name, surname and email should map directly\n",
    "# to the dictionary keys\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "article_file = \"./data/exampleResearchArticle.xml\"\n",
    "\n",
    "\n",
    "def get_root(fname):\n",
    "    tree = ET.parse(fname)\n",
    "    return tree.getroot()\n",
    "\n",
    "\n",
    "def get_authors(root):\n",
    "    authors = []\n",
    "    for author in root.findall('./fm/bibl/aug/au'):\n",
    "        data = {\n",
    "                \"fnm\": None,\n",
    "                \"snm\": None,\n",
    "                \"email\": None\n",
    "        }\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        data[\"fnm\"] = author.find('fnm').text\n",
    "        data[\"snm\"] = author.find('snm').text\n",
    "        data[\"email\"] = author.find('email').text\n",
    "        \n",
    "        print data\n",
    "\n",
    "        authors.append(data)\n",
    "\n",
    "    return authors\n",
    "\n",
    "\n",
    "def test():\n",
    "    solution = [{'fnm': 'Omer', 'snm': 'Mei-Dan', 'email': 'omer@extremegate.com'}, {'fnm': 'Mike', 'snm': 'Carmont', 'email': 'mcarmont@hotmail.com'}, {'fnm': 'Lior', 'snm': 'Laver', 'email': 'laver17@gmail.com'}, {'fnm': 'Meir', 'snm': 'Nyska', 'email': 'nyska@internet-zahav.net'}, {'fnm': 'Hagay', 'snm': 'Kammar', 'email': 'kammarh@gmail.com'}, {'fnm': 'Gideon', 'snm': 'Mann', 'email': 'gideon.mann.md@gmail.com'}, {'fnm': 'Barnaby', 'snm': 'Clarck', 'email': 'barns.nz@gmail.com'}, {'fnm': 'Eugene', 'snm': 'Kots', 'email': 'eukots@gmail.com'}]\n",
    "    \n",
    "    root = get_root(article_file)\n",
    "    data = get_authors(root)\n",
    "\n",
    "    assert data[0] == solution[0]\n",
    "    assert data[1][\"fnm\"] == solution[1][\"fnm\"]\n",
    "\n",
    "\n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Biomed iid fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'insr': ['I1'], 'fnm': 'Omer', 'snm': 'Mei-Dan', 'email': 'omer@extremegate.com'}\n",
      "{'insr': ['I2'], 'fnm': 'Mike', 'snm': 'Carmont', 'email': 'mcarmont@hotmail.com'}\n",
      "{'insr': ['I3', 'I4'], 'fnm': 'Lior', 'snm': 'Laver', 'email': 'laver17@gmail.com'}\n",
      "{'insr': ['I3'], 'fnm': 'Meir', 'snm': 'Nyska', 'email': 'nyska@internet-zahav.net'}\n",
      "{'insr': ['I8'], 'fnm': 'Hagay', 'snm': 'Kammar', 'email': 'kammarh@gmail.com'}\n",
      "{'insr': ['I3', 'I5'], 'fnm': 'Gideon', 'snm': 'Mann', 'email': 'gideon.mann.md@gmail.com'}\n",
      "{'insr': ['I6'], 'fnm': 'Barnaby', 'snm': 'Clarck', 'email': 'barns.nz@gmail.com'}\n",
      "{'insr': ['I7'], 'fnm': 'Eugene', 'snm': 'Kots', 'email': 'eukots@gmail.com'}\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# Your task here is to extract data from xml on authors of an article\n",
    "# and add it to a list, one item for an author.\n",
    "# See the provided data structure for the expected format.\n",
    "# The tags for first name, surname and email should map directly\n",
    "# to the dictionary keys, but you have to extract the attributes from the \"insr\" tag\n",
    "# and add them to the list for the dictionary key \"insr\"\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "article_file = \"./data/exampleResearchArticle.xml\"\n",
    "\n",
    "\n",
    "def get_root(fname):\n",
    "    tree = ET.parse(fname)\n",
    "    return tree.getroot()\n",
    "\n",
    "\n",
    "def get_authors(root):\n",
    "    authors = []\n",
    "    for author in root.findall('./fm/bibl/aug/au'):\n",
    "        data = {\n",
    "                \"fnm\": None,\n",
    "                \"snm\": None,\n",
    "                \"email\": None,\n",
    "                \"insr\": []\n",
    "        }\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        data[\"fnm\"] = author.find('fnm').text\n",
    "        data[\"snm\"] = author.find('snm').text\n",
    "        data[\"email\"] = author.find('email').text\n",
    "        \n",
    "        data[\"insr\"] = []\n",
    "        insrlist = author.findall('insr')\n",
    "        \n",
    "        for item in insrlist:\n",
    "            data[\"insr\"].append(item.attrib['iid'])\n",
    "        \n",
    "        print data\n",
    "\n",
    "        authors.append(data)\n",
    "\n",
    "    return authors\n",
    "\n",
    "\n",
    "def test():\n",
    "    solution = [{'insr': ['I1'], 'fnm': 'Omer', 'snm': 'Mei-Dan', 'email': 'omer@extremegate.com'},\n",
    "                {'insr': ['I2'], 'fnm': 'Mike', 'snm': 'Carmont', 'email': 'mcarmont@hotmail.com'},\n",
    "                {'insr': ['I3', 'I4'], 'fnm': 'Lior', 'snm': 'Laver', 'email': 'laver17@gmail.com'},\n",
    "                {'insr': ['I3'], 'fnm': 'Meir', 'snm': 'Nyska', 'email': 'nyska@internet-zahav.net'},\n",
    "                {'insr': ['I8'], 'fnm': 'Hagay', 'snm': 'Kammar', 'email': 'kammarh@gmail.com'},\n",
    "                {'insr': ['I3', 'I5'], 'fnm': 'Gideon', 'snm': 'Mann', 'email': 'gideon.mann.md@gmail.com'},\n",
    "                {'insr': ['I6'], 'fnm': 'Barnaby', 'snm': 'Clarck', 'email': 'barns.nz@gmail.com'},\n",
    "                {'insr': ['I7'], 'fnm': 'Eugene', 'snm': 'Kots', 'email': 'eukots@gmail.com'}]\n",
    "\n",
    "    root = get_root(article_file)\n",
    "    data = get_authors(root)\n",
    "\n",
    "    assert data[0] == solution[0]\n",
    "    assert data[1][\"insr\"] == solution[1][\"insr\"]\n",
    "\n",
    "\n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transtats - BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eventvalidation: /wEWjAkCoI\n",
      "Viewstate: /wEPDwUKLT\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "# Please note that the function 'make_request' is provided for your reference only.\n",
    "# You will not be able to to actually use it from within the Udacity web UI.\n",
    "# Your task is to process the HTML using BeautifulSoup, extract the hidden\n",
    "# form field values for \"__EVENTVALIDATION\" and \"__VIEWSTATE\" and set the appropriate\n",
    "# values in the data dictionary.\n",
    "# All your changes should be in the 'extract_data' function\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import json\n",
    "\n",
    "html_page = \"./data/page_source.html\"\n",
    "\n",
    "def extract_data(page):\n",
    "    data = {\"eventvalidation\": \"\",\n",
    "            \"viewstate\": \"\"}\n",
    "    with open(page, \"r\") as html:\n",
    "        # do something here to find the necessary values\n",
    "        soup = BeautifulSoup(html, \"lxml\")\n",
    "        data[\"viewstate\"] = soup.find(id=\"__VIEWSTATE\")['value']\n",
    "        data[\"eventvalidation\"] = soup.find(id=\"__EVENTVALIDATION\")['value']\n",
    "    print 'Eventvalidation: '+data[\"eventvalidation\"][:10]\n",
    "    print 'Viewstate: '+data[\"viewstate\"][:10]\n",
    "    return data\n",
    "\n",
    "\n",
    "def make_request(data):\n",
    "    eventvalidation = data[\"eventvalidation\"]\n",
    "    viewstate = data[\"viewstate\"]\n",
    "\n",
    "    r = requests.post(\"http://www.transtats.bts.gov/Data_Elements.aspx?Data=2\",\n",
    "                    data={'AirportList': \"BOS\",\n",
    "                          'CarrierList': \"VX\",\n",
    "                          'Submit': 'Submit',\n",
    "                          \"__EVENTTARGET\": \"\",\n",
    "                          \"__EVENTARGUMENT\": \"\",\n",
    "                          \"__EVENTVALIDATION\": eventvalidation,\n",
    "                          \"__VIEWSTATE\": viewstate\n",
    "                    })\n",
    "\n",
    "    return r.text\n",
    "\n",
    "\n",
    "def test():\n",
    "    data = extract_data(html_page)\n",
    "    assert data[\"eventvalidation\"] != \"\"\n",
    "    assert data[\"eventvalidation\"].startswith(\"/wEWjAkCoIj1ng0\")\n",
    "    assert data[\"viewstate\"].startswith(\"/wEPDwUKLTI\")\n",
    "    \n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 4 - Problem Set: Data in More Complex Formats\n",
    "## Carrier List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['FL', 'AS', 'AA', 'MQ', '5Y', 'DL', 'EV', 'F9', 'HA', 'B6', 'OO', 'WN', 'NK', 'US', 'UA', 'VX']\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Your task in this exercise is to modify 'extract_carrier()` to get a list of\n",
    "all airlines. Exclude all of the combination values like \"All U.S. Carriers\"\n",
    "from the data that you return. You should return a list of codes for the\n",
    "carriers.\n",
    "\n",
    "All your changes should be in the 'extract_carrier()' function. The\n",
    "'options.html' file in the tab above is a stripped down version of what is\n",
    "actually on the website, but should provide an example of what you should get\n",
    "from the full file.\n",
    "\n",
    "Please note that the function 'make_request()' is provided for your reference\n",
    "only. You will not be able to to actually use it from within the Udacity web UI.\n",
    "\"\"\"\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "html_page = \"./data/options.html\"\n",
    "\n",
    "\n",
    "def extract_carriers(page):\n",
    "    data = []\n",
    "\n",
    "    with open(page, \"r\") as html:\n",
    "        # do something here to find the necessary values\n",
    "        soup = BeautifulSoup(html, \"lxml\")\n",
    "        carriers = soup.find(id=\"CarrierList\").find_all('option')\n",
    "        for carrier in carriers:\n",
    "            if not 'All' in carrier['value']:\n",
    "                data.append(carrier['value'])\n",
    "                #print carrier['value']\n",
    "    print data\n",
    "    return data\n",
    "\n",
    "\n",
    "def make_request(data):\n",
    "    eventvalidation = data[\"eventvalidation\"]\n",
    "    viewstate = data[\"viewstate\"]\n",
    "    airport = data[\"airport\"]\n",
    "    carrier = data[\"carrier\"]\n",
    "\n",
    "    r = s.post(\"https://www.transtats.bts.gov/Data_Elements.aspx?Data=2\",\n",
    "               data = ((\"__EVENTTARGET\", \"\"),\n",
    "                       (\"__EVENTARGUMENT\", \"\"),\n",
    "                       (\"__VIEWSTATE\", viewstate),\n",
    "                       (\"__VIEWSTATEGENERATOR\",viewstategenerator),\n",
    "                       (\"__EVENTVALIDATION\", eventvalidation),\n",
    "                       (\"CarrierList\", carrier),\n",
    "                       (\"AirportList\", airport),\n",
    "                       (\"Submit\", \"Submit\")))\n",
    "\n",
    "    return r.text\n",
    "\n",
    "def test():\n",
    "    data = extract_carriers(html_page)\n",
    "    assert len(data) == 16\n",
    "    assert \"FL\" in data\n",
    "    assert \"NK\" in data\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Airport List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ATL', 'BWI', 'BOS', 'CLT', 'MDW', 'ORD', 'DFW', 'DEN', 'DTW', 'FLL', 'IAH', 'LAS', 'LAX', 'ABR', 'ABI']\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Complete the 'extract_airports()' function so that it returns a list of airport\n",
    "codes, excluding any combinations like \"All\".\n",
    "\n",
    "Refer to the 'options.html' file in the tab above for a stripped down version\n",
    "of what is actually on the website. The test() assertions are based on the\n",
    "given file.\n",
    "\"\"\"\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "html_page = \"./data/options.html\"\n",
    "\n",
    "\n",
    "def extract_airports(page):\n",
    "    data = []\n",
    "    with open(page, \"r\") as html:\n",
    "        # do something here to find the necessary values\n",
    "        soup = BeautifulSoup(html, \"lxml\")\n",
    "        airports = soup.find(id=\"AirportList\").find_all('option')\n",
    "        for airport in airports:\n",
    "            if not 'All' in airport['value']:\n",
    "                data.append(airport['value'])\n",
    "    print data\n",
    "    return data\n",
    "\n",
    "\n",
    "def test():\n",
    "    data = extract_airports(html_page)\n",
    "    assert len(data) == 15\n",
    "    assert \"ATL\" in data\n",
    "    assert \"ABR\" in data\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running a simple test...\n",
      "[{'airport': 'BWI', 'month': 10, 'flights': {'international': 92565, 'domestic': 815489}, 'courier': 'AS', 'year': 2002}, {'airport': 'BWI', 'month': 11, 'flights': {'international': 91342, 'domestic': 766775}, 'courier': 'AS', 'year': 2002}, {'airport': 'BWI', 'month': 12, 'flights': {'international': 96881, 'domestic': 782175}, 'courier': 'AS', 'year': 2002}, {'airport': 'BWI', 'month': 1, 'flights': {'international': 98053, 'domestic': 785651}, 'courier': 'AS', 'year': 2003}, {'airport': 'BWI', 'month': 2, 'flights': {'international': 85965, 'domestic': 690750}, 'courier': 'AS', 'year': 2003}, {'airport': 'BWI', 'month': 3, 'flights': {'international': 97929, 'domestic': 797634}, 'courier': 'AS', 'year': 2003}, {'airport': 'BWI', 'month': 4, 'flights': {'international': 89398, 'domestic': 766639}, 'courier': 'AS', 'year': 2003}, {'airport': 'BWI', 'month': 5, 'flights': {'international': 87671, 'domestic': 789857}, 'courier': 'AS', 'year': 2003}, {'airport': 'BWI', 'month': 6, 'flights': {'international': 95435, 'domestic': 798841}, 'courier': 'AS', 'year': 2003}, {'airport': 'BWI', 'month': 7, 'flights': {'international': 102795, 'domestic': 832075}, 'courier': 'AS', 'year': 2003}, {'airport': 'BWI', 'month': 8, 'flights': {'international': 102145, 'domestic': 831185}, 'courier': 'AS', 'year': 2003}, {'airport': 'BWI', 'month': 9, 'flights': {'international': 90681, 'domestic': 782264}, 'courier': 'AS', 'year': 2003}, {'airport': 'BWI', 'month': 10, 'flights': {'international': 91820, 'domestic': 818777}, 'courier': 'AS', 'year': 2003}, {'airport': 'BWI', 'month': 11, 'flights': {'international': 91004, 'domestic': 766266}, 'courier': 'AS', 'year': 2003}, {'airport': 'BWI', 'month': 12, 'flights': {'international': 97094, 'domestic': 798879}, 'courier': 'AS', 'year': 2003}]\n",
      "[{'airport': 'ATL', 'month': 10, 'flights': {'international': 92565, 'domestic': 815489}, 'courier': 'FL', 'year': 2002}, {'airport': 'ATL', 'month': 11, 'flights': {'international': 91342, 'domestic': 766775}, 'courier': 'FL', 'year': 2002}, {'airport': 'ATL', 'month': 12, 'flights': {'international': 96881, 'domestic': 782175}, 'courier': 'FL', 'year': 2002}, {'airport': 'ATL', 'month': 1, 'flights': {'international': 98053, 'domestic': 785651}, 'courier': 'FL', 'year': 2003}, {'airport': 'ATL', 'month': 2, 'flights': {'international': 85965, 'domestic': 690750}, 'courier': 'FL', 'year': 2003}, {'airport': 'ATL', 'month': 3, 'flights': {'international': 97929, 'domestic': 797634}, 'courier': 'FL', 'year': 2003}, {'airport': 'ATL', 'month': 4, 'flights': {'international': 89398, 'domestic': 766639}, 'courier': 'FL', 'year': 2003}, {'airport': 'ATL', 'month': 5, 'flights': {'international': 87671, 'domestic': 789857}, 'courier': 'FL', 'year': 2003}, {'airport': 'ATL', 'month': 6, 'flights': {'international': 95435, 'domestic': 798841}, 'courier': 'FL', 'year': 2003}, {'airport': 'ATL', 'month': 7, 'flights': {'international': 102795, 'domestic': 832075}, 'courier': 'FL', 'year': 2003}, {'airport': 'ATL', 'month': 8, 'flights': {'international': 102145, 'domestic': 831185}, 'courier': 'FL', 'year': 2003}, {'airport': 'ATL', 'month': 9, 'flights': {'international': 90681, 'domestic': 782264}, 'courier': 'FL', 'year': 2003}, {'airport': 'ATL', 'month': 10, 'flights': {'international': 91820, 'domestic': 818777}, 'courier': 'FL', 'year': 2003}, {'airport': 'ATL', 'month': 11, 'flights': {'international': 91004, 'domestic': 766266}, 'courier': 'FL', 'year': 2003}, {'airport': 'ATL', 'month': 12, 'flights': {'international': 97094, 'domestic': 798879}, 'courier': 'FL', 'year': 2003}]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-113-58152a39a53c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"__main__\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 138\u001b[1;33m     \u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-113-58152a39a53c>\u001b[0m in \u001b[0;36mtest\u001b[1;34m()\u001b[0m\n\u001b[0;32m    128\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mentry\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"airport\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mentry\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"courier\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 130\u001b[1;33m     \u001b[1;32massert\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"courier\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'FL'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    131\u001b[0m     \u001b[1;32massert\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"month\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m     \u001b[1;32massert\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"airport\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"ATL\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Let's assume that you combined the code from the previous 2 exercises with code\n",
    "from the lesson on how to build requests, and downloaded all the data locally.\n",
    "The files are in a directory \"data\", named after the carrier and airport:\n",
    "\"{}-{}.html\".format(carrier, airport), for example \"FL-ATL.html\".\n",
    "\n",
    "The table with flight info has a table class=\"dataTDRight\". Your task is to\n",
    "use 'process_file()' to extract the flight data from that table as a list of\n",
    "dictionaries, each dictionary containing relevant data from the file and table\n",
    "row. This is an example of the data structure you should return:\n",
    "\n",
    "data = [{\"courier\": \"FL\",\n",
    "         \"airport\": \"ATL\",\n",
    "         \"year\": 2012,\n",
    "         \"month\": 12,\n",
    "         \"flights\": {\"domestic\": 100,\n",
    "                     \"international\": 100}\n",
    "        },\n",
    "         {\"courier\": \"...\"}\n",
    "]\n",
    "\n",
    "Note - year, month, and the flight data should be integers.\n",
    "You should skip the rows that contain the TOTAL data for a year.\n",
    "\n",
    "There are couple of helper functions to deal with the data files.\n",
    "Please do not change them for grading purposes.\n",
    "All your changes should be in the 'process_file()' function.\n",
    "\n",
    "The 'data/FL-ATL.html' file in the tab above is only a part of the full data,\n",
    "covering data through 2003. The test() code will be run on the full table, but\n",
    "the given file should provide an example of what you will get.\n",
    "\"\"\"\n",
    "from bs4 import BeautifulSoup\n",
    "from zipfile import ZipFile\n",
    "import os\n",
    "\n",
    "datadir = \"data_airport\"\n",
    "\n",
    "\n",
    "def open_zip(datadir):\n",
    "    with ZipFile('{0}.zip'.format(datadir), 'r') as myzip:\n",
    "        myzip.extractall()\n",
    "\n",
    "\n",
    "def process_all(datadir):\n",
    "    files = os.listdir(datadir)\n",
    "    return files\n",
    "\n",
    "\n",
    "def process_file(f):\n",
    "    \"\"\"\n",
    "    This function extracts data from the file given as the function argument in\n",
    "    a list of dictionaries. This is example of the data structure you should\n",
    "    return:\n",
    "\n",
    "    data = [{\"courier\": \"FL\",\n",
    "             \"airport\": \"ATL\",\n",
    "             \"year\": 2012,\n",
    "             \"month\": 12,\n",
    "             \"flights\": {\"domestic\": 100,\n",
    "                         \"international\": 100}\n",
    "            },\n",
    "            {\"courier\": \"...\"}\n",
    "    ]\n",
    "\n",
    "\n",
    "    Note - year, month, and the flight data should be integers.\n",
    "    You should skip the rows that contain the TOTAL data for a year.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    info = {}\n",
    "    info[\"courier\"], info[\"airport\"] = f[:6].split(\"-\")\n",
    "    # Note: create a new dictionary for each entry in the output data list.\n",
    "    # If you use the info dictionary defined here each element in the list \n",
    "    # will be a reference to the same info dictionary.\n",
    "    with open(\"{}/{}\".format(datadir, f), \"r\") as html:\n",
    "\n",
    "        soup = BeautifulSoup(html, \"lxml\")\n",
    "        table = soup.find(\"table\", id=\"DataGrid1\")\n",
    "        #items = table.find_all(\"td\")\n",
    "        #for item in items:\n",
    "        #    print item.find(\"td\")\n",
    "        rawdata = []\n",
    "        for node in table.findAll('td'):\n",
    "            rawdata.append(''.join(node.findAll(text=True)))\n",
    "        #print rawdata\n",
    "              \n",
    "        for i in range(5, len(rawdata), 5):\n",
    "            data_dict = {\"courier\": info[\"courier\"],\n",
    "                 \"airport\": info[\"airport\"],\n",
    "                 \"year\": None,\n",
    "                 \"month\": None,\n",
    "                 \"flights\": {\"domestic\": None,\n",
    "                             \"international\": None}\n",
    "                }\n",
    "            if not rawdata[i+1] == 'TOTAL':\n",
    "                data_dict[\"year\"] = int(rawdata[i])                \n",
    "                data_dict[\"month\"] = int(rawdata[i+1])\n",
    "                #print int(rawdata[i+2].replace(',',''))\n",
    "                data_dict[\"flights\"][\"domestic\"] = int(rawdata[i+2].replace(',',''))\n",
    "                data_dict[\"flights\"][\"international\"] = int(rawdata[i+3].replace(',',''))\n",
    "                \n",
    "                #print data_dict\n",
    "                data.append(data_dict)\n",
    "        \n",
    "    print data\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def test():\n",
    "    print \"Running a simple test...\"\n",
    "    #open_zip(datadir)\n",
    "    files = process_all(datadir)\n",
    "    data = []\n",
    "    # Test will loop over three data files.\n",
    "    for f in files:\n",
    "        data += process_file(f)\n",
    "    \n",
    "    #assert len(data) == 399  # Total number of rows\n",
    "    for entry in data[:3]:\n",
    "        #print type(entry[\"year\"])\n",
    "        assert type(entry[\"year\"]) == int\n",
    "        assert type(entry[\"month\"]) == int\n",
    "        assert type(entry[\"flights\"][\"domestic\"]) == int\n",
    "        assert len(entry[\"airport\"]) == 3\n",
    "        assert len(entry[\"courier\"]) == 2\n",
    "    assert data[0][\"courier\"] == 'FL'\n",
    "    assert data[0][\"month\"] == 10\n",
    "    assert data[-1][\"airport\"] == \"ATL\"\n",
    "    assert data[-1][\"flights\"] == {'international': 108289, 'domestic': 701425}\n",
    "    \n",
    "    print \"... success!\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Patent Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "657\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "This and the following exercise are using US Patent database. The patent.data\n",
    "file is a small excerpt of much larger datafiles that are available for\n",
    "download from US Patent website. These files are pretty large ( >100 MB each).\n",
    "The original file is ~600MB large, you might not be able to open it in a text\n",
    "editor.\n",
    "\n",
    "The data itself is in XML, however there is a problem with how it's formatted.\n",
    "Please run this script and observe the error. Then find the line that is\n",
    "causing the error. You can do that by just looking at the datafile in the web\n",
    "UI, or programmatically. For quiz purposes it does not matter, but as an\n",
    "exercise we suggest that you try to do it programmatically.\n",
    "\n",
    "NOTE: You do not need to correct the error - for now, just find where the error\n",
    "is occurring.\n",
    "\"\"\"\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "PATENTS = './data/patent.data'\n",
    "\n",
    "def get_root(fname):\n",
    "    #tree = []\n",
    "    #line = 0\n",
    "    #for event, element in ET.iterparse(fname):\n",
    "    #    line += 1\n",
    "    #    print line\n",
    "    #    tree.append(element)\n",
    "    \n",
    "    try:\n",
    "        tree = ET.parse(fname)\n",
    "    except ET.ParseError as err:\n",
    "        lineno, column = err.position\n",
    "        #print lineno\n",
    "    \n",
    "    return lineno\n",
    "\n",
    "\n",
    "line = get_root(PATENTS)\n",
    "print line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Patents - Split .xml files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "# So, the problem is that the gigantic file is actually not a valid XML, because\n",
    "# it has several root elements, and XML declarations.\n",
    "# It is, a matter of fact, a collection of a lot of concatenated XML documents.\n",
    "# So, one solution would be to split the file into separate documents,\n",
    "# so that you can process the resulting files as valid XML documents.\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "PATENTS = './data/patent.data'\n",
    "\n",
    "def get_root(fname):\n",
    "    tree = ET.parse(fname)\n",
    "    return tree.getroot()\n",
    "\n",
    "\n",
    "def split_file(filename):\n",
    "    \"\"\"\n",
    "    Split the input file into separate files, each containing a single patent.\n",
    "    As a hint - each patent declaration starts with the same line that was\n",
    "    causing the error found in the previous exercises.\n",
    "    \n",
    "    The new files should be saved with filename in the following format:\n",
    "    \"{}-{}\".format(filename, n) where n is a counter, starting from 0.\n",
    "    \"\"\"\n",
    "\n",
    "    # Open file to read\n",
    "    with open(filename, \"r\") as r:\n",
    "\n",
    "        # Counter, initial value -1 in order to have 0 as first file index\n",
    "        n=-1\n",
    "\n",
    "        # Start reading file line by line\n",
    "        for i, line in enumerate(r):\n",
    "\n",
    "            # If '?xml' is found in line, increase counter n\n",
    "            if '?xml' in line:\n",
    "                n+=1              \n",
    "\n",
    "            # Write lines to file    \n",
    "            with open(\"{}-{}\".format(PATENTS, n), \"a\") as f:\n",
    "                f.write(line)\n",
    "\n",
    "def test():\n",
    "    split_file(PATENTS)\n",
    "    for n in range(4):\n",
    "        try:\n",
    "            fname = \"{}-{}\".format(PATENTS, n)\n",
    "            f = open(fname, \"r\")\n",
    "            if not f.readline().startswith(\"<?xml\"):\n",
    "                print \"You have not split the file {} in the correct boundary!\".format(fname)\n",
    "            f.close()\n",
    "        except:\n",
    "            print \"Could not find file {}. Check if the filename is correct!\".format(fname)\n",
    "\n",
    "\n",
    "test()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
